{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b55f103",
   "metadata": {},
   "source": [
    "# MNIST Data Image Recognition\n",
    "\n",
    "In this notebook we propose a simple Neural Network (NN) architecture to recognize hand-written digits. We use the MNIST dataset, consisting of 70,000 images split into 60,000 for training and 10,000 for validation.\n",
    "\n",
    "The aim of this project is two-fold. First, to emphasize the importance of the choice of hyperparameters and regularization in NNs, starting with a simple fully connected neural network (FCNN) using the well-known MNIST dataset as a prototypical benchmark. Second, to assert the power of Convolutional Neural Networks (CNNs) for image recognition tasks, which significantly outperform simpler NN architectures for this type of tasks.\n",
    "\n",
    "Starting with the FCNN, following the literature on this task we begin with a simple benchmark architecture, consisting of one hidden-layer with 128 nodes, a ReLU activation function, and an output softmax activation function for the output layer, combined with a cross-validation entropy loss-function, the standard choice for non-binary classification. We also opt for an ADAM optimizer with default 0.001 learning rate, and 32-sized mini-batches. We compute the loss function and accuracy on both the training and validation sets to assess the efficiency of the NN.\n",
    "\n",
    "We then perform a hyperparameter search over various hyperparameters, evaluating the error on the validation set for each combination of hyperparameters and we select the most accurate ones. Considering the simplicity of the images involved we opt to stick with having just one hidden layer, and vary the other hyperparameters as follows:\n",
    "\n",
    "- learning rates = 0.0001, 0.001, 0.01\n",
    "- batch sizes = 32, 64, 128\n",
    "- epochs = 10, 20, 50\n",
    "- hidden units = 64, 128, 256\n",
    "- activations = relu, tanh\n",
    "- optimizers = adam, sgd, rmsprop\n",
    "\n",
    "We train and test all 486 combinations of these hyperparameters and select the 5 combinations which yield the best accuracy on the validation set. We then add regularization to the picture to reduce overiftting, and compare the performances of the various models. \n",
    "Here we opt for a grid search, rather than a randomized one, in order to better visualize how the accuracy of the model changes, as a pedagogical introduction to the topic.\n",
    "\n",
    "We then move on to a CNN architecture to tackle the same problem. Again, we start with a benchmark model and then perform a hyperparameter search, now starting with a randomized hyperparameter search from the start, and compare the performace of the two types of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6065a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner as kt\n",
    "import itertools\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b6a7735",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 60000\n",
      "Image dimensions: 28x28\n",
      "First label: 5 \n",
      " \n",
      "Dimension of array of training images: (60000, 28, 28)\n",
      "Dimension of array of training labels: (60000,)\n",
      "So, our training set has 60000 examples, with 28x28 pixels per example\n",
      "Flattening out the image from a 28x28 array to a 784-dimensional vector\n",
      "\n",
      "Flattened training images shape: (60000, 784)\n",
      "Flattened test images shape: (10000, 784)\n",
      "\n",
      "Training images labels shape: (60000,)\n",
      "Test images labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset from tf and reshape\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Print dataset statistics\n",
    "print(f\"Number of training images: {train_images.shape[0]}\")\n",
    "print(f\"Image dimensions: {train_images.shape[1]}x{train_images.shape[2]}\")\n",
    "print(f\"First label: {train_labels[0]} \\n \")\n",
    "\n",
    "# Normalize the image data RGB\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "# Print shape of train and test images, before and after reshaping\n",
    "print(f\"Dimension of array of training images: {train_images.shape}\")\n",
    "print(f\"Dimension of array of training labels: {train_labels.shape}\")\n",
    "print(f\"So, our training set has {train_images.shape[0]} examples, with {train_images.shape[1]}x{train_images.shape[2]} pixels per example\")\n",
    "print(f\"Flattening out the image from a {train_images.shape[1]}x{train_images.shape[2]} array to a {train_images.shape[1]*train_images.shape[2]}-dimensional vector\")\n",
    "\n",
    "\n",
    "# Flatten each 28x28 image into a vector of size 784 (28*28)\n",
    "x_train = train_images.reshape(-1, train_images.shape[1] * train_images.shape[2])\n",
    "x_test = test_images.reshape(-1, test_images.shape[1] * test_images.shape[2])\n",
    "\n",
    "y_train = train_labels\n",
    "y_test = test_labels\n",
    "\n",
    "\n",
    "# Print the new shape to verify\n",
    "print(f\"\\nFlattened training images shape: {x_train.shape}\")\n",
    "print(f\"Flattened test images shape: {x_test.shape}\")\n",
    "\n",
    "print(f\"\\nTraining images labels shape: {y_train.shape}\")\n",
    "print(f\"Test images labels shape: {y_test.shape}\")\n",
    "\n",
    "# Seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bd9676f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test plotting  loaded images\n",
    "plt.imshow(train_images[0], cmap='gray')\n",
    "plt.title(f\"Label: {train_labels[0]}\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.imshow(test_images[5000], cmap='gray')\n",
    "plt.title(f\"Label: {test_labels[5000]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3316c8",
   "metadata": {},
   "source": [
    "As we previously stated, we begin with a benchmark NN with the following details:\n",
    "- one hidden layer, 128 nodes, ReLU activation function\n",
    "- output layer, 10 nodes, softmax activation function, cross-entropy loss function\n",
    "- ADAM optimizer, 0.001 learning rate (default)\n",
    "- 32-sized mini-batches (default)\n",
    "- 10 training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc523de0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m100,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,770</span> (397.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m101,770\u001b[0m (397.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,770</span> (397.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m101,770\u001b[0m (397.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.8792 - loss: 0.4239 - val_accuracy: 0.9588 - val_loss: 0.1345\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9645 - loss: 0.1221 - val_accuracy: 0.9699 - val_loss: 0.0983\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9765 - loss: 0.0788 - val_accuracy: 0.9726 - val_loss: 0.0868\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9832 - loss: 0.0564 - val_accuracy: 0.9745 - val_loss: 0.0817\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9877 - loss: 0.0414 - val_accuracy: 0.9758 - val_loss: 0.0810\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9919 - loss: 0.0307 - val_accuracy: 0.9755 - val_loss: 0.0837\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9945 - loss: 0.0229 - val_accuracy: 0.9760 - val_loss: 0.0814\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9953 - loss: 0.0185 - val_accuracy: 0.9774 - val_loss: 0.0876\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9964 - loss: 0.0146 - val_accuracy: 0.9733 - val_loss: 0.1002\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9969 - loss: 0.0118 - val_accuracy: 0.9767 - val_loss: 0.0896\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9725 - loss: 0.0999     \n",
      "Final training accuracy: 0.9969\n",
      "Final dev accuracy: 0.9767\n",
      "\n",
      "Final training error: 0.30666589736938477 %\n",
      "Final dev error: 2.3299992084503174 %\n"
     ]
    }
   ],
   "source": [
    "# Benchmark Neural Network (without regularization)\n",
    "\n",
    "\n",
    "# Number of nodes in hidden and output layers\n",
    "n1 = 128\n",
    "nout = 10\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Dense(n1, activation='relu', input_shape=(28*28,)),\n",
    "    layers.Dense(nout, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Compile the model with ADAM\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model and save the training history\n",
    "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "# Plot the loss function\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Function vs Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(x_test, y_test)\n",
    "\n",
    "training_error = (1 - history.history['accuracy'][-1])\n",
    "dev_error = (1 - history.history['val_accuracy'][-1])\n",
    "\n",
    "print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final dev accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal training error: {training_error*100} %\")\n",
    "print(f\"Final dev error: {dev_error*100} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f8eaf9",
   "metadata": {},
   "source": [
    "We find the following performance for the benchmark NN:\n",
    "- Final training error: 0.31 %\n",
    "- Final dev error: 2.33 %\n",
    "The variance amounts to 2.02%. This can be anticipated by considering the plots of the loss function vs epochs for the training set, which decreases steadily, and for the validation set, which plateaus after ~4 epochs. This could be due to overfitting. We will address this later by employing regularization. \n",
    "\n",
    "We now explore the model more thoroughly by performing a systematic search of hyperparameters over a grid (takes some time to run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0f91558",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=0.0001, batch_size=32, epochs=10, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=32, epochs=10, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=32, epochs=10, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=32, epochs=10, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=32, epochs=10, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=32, epochs=10, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=32, epochs=10, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=32, epochs=10, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=32, epochs=10, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=32, epochs=10, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=32, epochs=10, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=32, epochs=10, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=32, epochs=10, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=32, epochs=10, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=32, epochs=10, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=32, epochs=10, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=32, epochs=10, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=32, epochs=10, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=32, epochs=20, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=32, epochs=20, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=32, epochs=20, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=32, epochs=20, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=32, epochs=20, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=32, epochs=20, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=32, epochs=20, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=32, epochs=20, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=32, epochs=20, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=32, epochs=20, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=32, epochs=20, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=32, epochs=20, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=32, epochs=20, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=32, epochs=20, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=32, epochs=20, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=32, epochs=20, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=32, epochs=20, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=32, epochs=20, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=32, epochs=50, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=32, epochs=50, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=32, epochs=50, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=32, epochs=50, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=32, epochs=50, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=32, epochs=50, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=32, epochs=50, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=32, epochs=50, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=32, epochs=50, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=32, epochs=50, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=32, epochs=50, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=32, epochs=50, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=32, epochs=50, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=32, epochs=50, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=32, epochs=50, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=32, epochs=50, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=32, epochs=50, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=32, epochs=50, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=64, epochs=10, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=64, epochs=10, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=64, epochs=10, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=64, epochs=10, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=64, epochs=10, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=64, epochs=10, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=64, epochs=10, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=64, epochs=10, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=64, epochs=10, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=64, epochs=10, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=64, epochs=10, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=64, epochs=10, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=64, epochs=10, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=64, epochs=10, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=64, epochs=10, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=64, epochs=10, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=64, epochs=10, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=64, epochs=10, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=64, epochs=20, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=64, epochs=20, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=64, epochs=20, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=64, epochs=20, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=64, epochs=20, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=64, epochs=20, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=64, epochs=20, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=64, epochs=20, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=64, epochs=20, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=64, epochs=20, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=64, epochs=20, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=64, epochs=20, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=64, epochs=20, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=64, epochs=20, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=64, epochs=20, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=64, epochs=20, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=64, epochs=20, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=64, epochs=20, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=256, activation=tanh, optimizer=rmsprop\n",
      "\n",
      "Top 5 hyperparameter choices:\n",
      "\n",
      "Rank 1:\n",
      "Learning rate: 0.01\n",
      "Batch size: 64\n",
      "Epochs: 50\n",
      "Hidden units: 256\n",
      "Activation: tanh\n",
      "Optimizer: rmsprop\n",
      "Validation accuracy: 0.9833\n",
      "Dev error: 1.67%\n",
      "\n",
      "Rank 2:\n",
      "Learning rate: 0.001\n",
      "Batch size: 32\n",
      "Epochs: 50\n",
      "Hidden units: 256\n",
      "Activation: tanh\n",
      "Optimizer: rmsprop\n",
      "Validation accuracy: 0.9832\n",
      "Dev error: 1.68%\n",
      "\n",
      "Rank 3:\n",
      "Learning rate: 0.0001\n",
      "Batch size: 128\n",
      "Epochs: 50\n",
      "Hidden units: 256\n",
      "Activation: relu\n",
      "Optimizer: adam\n",
      "Validation accuracy: 0.9830\n",
      "Dev error: 1.70%\n",
      "\n",
      "Rank 4:\n",
      "Learning rate: 0.01\n",
      "Batch size: 64\n",
      "Epochs: 50\n",
      "Hidden units: 256\n",
      "Activation: relu\n",
      "Optimizer: adam\n",
      "Validation accuracy: 0.9829\n",
      "Dev error: 1.71%\n",
      "\n",
      "Rank 5:\n",
      "Learning rate: 0.0001\n",
      "Batch size: 32\n",
      "Epochs: 50\n",
      "Hidden units: 256\n",
      "Activation: tanh\n",
      "Optimizer: rmsprop\n",
      "Validation accuracy: 0.9828\n",
      "Dev error: 1.72%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=0.0001, batch_size=64, epochs=20, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=64, epochs=20, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=64, epochs=20, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=64, epochs=50, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=10, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=20, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.0001, batch_size=128, epochs=50, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=128, activation=tanh, optimizer=rmsprop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=0.001, batch_size=32, epochs=10, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=10, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=20, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=32, epochs=50, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=10, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=20, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=128, activation=tanh, optimizer=adam\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=0.001, batch_size=64, epochs=50, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=64, epochs=50, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=10, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=20, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.001, batch_size=128, epochs=50, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=10, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=128, activation=relu, optimizer=sgd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=0.01, batch_size=32, epochs=20, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=20, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=32, epochs=50, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=10, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=20, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=64, epochs=50, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=128, activation=relu, optimizer=adam\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=0.01, batch_size=128, epochs=10, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=10, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=20, units=256, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=64, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=64, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=64, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=64, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=64, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=64, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=128, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=128, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=128, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=128, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=128, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=128, activation=tanh, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=256, activation=relu, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=256, activation=relu, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=256, activation=relu, optimizer=rmsprop\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=256, activation=tanh, optimizer=adam\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=256, activation=tanh, optimizer=sgd\n",
      "Training with lr=0.01, batch_size=128, epochs=50, units=256, activation=tanh, optimizer=rmsprop\n",
      "\n",
      "Top 5 hyperparameter choices:\n",
      "\n",
      "Rank 1:\n",
      "Learning rate: 0.0001\n",
      "Batch size: 32\n",
      "Epochs: 50\n",
      "Hidden units: 256\n",
      "Activation: tanh\n",
      "Optimizer: rmsprop\n",
      "Validation accuracy: 0.9841\n",
      "Dev error: 1.59%\n",
      "\n",
      "Rank 2:\n",
      "Learning rate: 0.01\n",
      "Batch size: 64\n",
      "Epochs: 50\n",
      "Hidden units: 256\n",
      "Activation: relu\n",
      "Optimizer: rmsprop\n",
      "Validation accuracy: 0.9840\n",
      "Dev error: 1.60%\n",
      "\n",
      "Rank 3:\n",
      "Learning rate: 0.001\n",
      "Batch size: 64\n",
      "Epochs: 50\n",
      "Hidden units: 256\n",
      "Activation: tanh\n",
      "Optimizer: rmsprop\n",
      "Validation accuracy: 0.9839\n",
      "Dev error: 1.61%\n",
      "\n",
      "Rank 4:\n",
      "Learning rate: 0.01\n",
      "Batch size: 64\n",
      "Epochs: 50\n",
      "Hidden units: 256\n",
      "Activation: tanh\n",
      "Optimizer: rmsprop\n",
      "Validation accuracy: 0.9837\n",
      "Dev error: 1.63%\n",
      "\n",
      "Rank 5:\n",
      "Learning rate: 0.0001\n",
      "Batch size: 128\n",
      "Epochs: 50\n",
      "Hidden units: 256\n",
      "Activation: relu\n",
      "Optimizer: rmsprop\n",
      "Validation accuracy: 0.9836\n",
      "Dev error: 1.64%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter grid\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "batch_sizes = [32, 64, 128]\n",
    "epochs = [10, 20, 50]\n",
    "hidden_units = [64, 128, 256]\n",
    "activations = ['relu', 'tanh']\n",
    "optimizers = ['adam', 'sgd', 'rmsprop']\n",
    "\n",
    "\n",
    "# Function to create and compile model\n",
    "def create_model(learning_rate, batch_size, hidden_units, activation, optimizer):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(hidden_units, activation=activation, input_shape=(28*28,)),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile model with given optimizer and learning rate\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "# Grid search\n",
    "for lr, batch, epoch, units, activation, optimizer in itertools.product(\n",
    "    learning_rates, batch_sizes, epochs, hidden_units, activations, optimizers):\n",
    "    \n",
    "    print(f\"Training with lr={lr}, batch_size={batch}, epochs={epoch}, units={units}, activation={activation}, optimizer={optimizer}\")\n",
    "    \n",
    "    model = create_model(lr, batch, units, activation, optimizer)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(x_train, y_train, \n",
    "                        epochs=epoch, \n",
    "                        batch_size=batch, \n",
    "                        validation_data=(x_test, y_test), \n",
    "                        verbose=0)\n",
    "    \n",
    "    # Evaluate model performance\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    \n",
    "    # Calculate dev error (1 - validation accuracy)\n",
    "    dev_error = 1 - val_accuracy\n",
    "    \n",
    "    # Append results\n",
    "    results.append({\n",
    "        'learning_rate': lr,\n",
    "        'batch_size': batch,\n",
    "        'epochs': epoch,\n",
    "        'hidden_units': units,\n",
    "        'activation': activation,\n",
    "        'optimizer': optimizer,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'dev_error': dev_error\n",
    "    })\n",
    "\n",
    "# Sort results by validation accuracy in descending order\n",
    "sorted_results = sorted(results, key=lambda x: x['val_accuracy'], reverse=True)\n",
    "\n",
    "# Print the top 5 results\n",
    "print(\"\\nTop 5 hyperparameter choices:\")\n",
    "for i, result in enumerate(sorted_results[:5]):\n",
    "    print(f\"\\nRank {i+1}:\")\n",
    "    print(f\"Learning rate: {result['learning_rate']}\")\n",
    "    print(f\"Batch size: {result['batch_size']}\")\n",
    "    print(f\"Epochs: {result['epochs']}\")\n",
    "    print(f\"Hidden units: {result['hidden_units']}\")\n",
    "    print(f\"Activation: {result['activation']}\")\n",
    "    print(f\"Optimizer: {result['optimizer']}\")\n",
    "    print(f\"Validation accuracy: {result['val_accuracy']:.4f}\")\n",
    "    print(f\"Dev error: {result['dev_error'] * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce827b19-03b4-4233-9e8a-f463fb9e77a3",
   "metadata": {},
   "source": [
    "NEW BEST 5:\n",
    "\n",
    "Rank 1:\n",
    "Learning rate: 0.01\n",
    "Batch size: 64\n",
    "Epochs: 50\n",
    "Hidden units: 256\n",
    "Activation: tanh\n",
    "Optimizer: rmsprop\n",
    "Validation accuracy: 0.9833\n",
    "Dev error: 1.67%\n",
    "\n",
    "Rank 2:\n",
    "Learning rate: 0.001\n",
    "Batch size: 32\n",
    "Epochs: 50\n",
    "Hidden units: 256\n",
    "Activation: tanh\n",
    "Optimizer: rmsprop\n",
    "Validation accuracy: 0.9832\n",
    "Dev error: 1.68%\n",
    "\n",
    "Rank 3:\n",
    "Learning rate: 0.0001\n",
    "Batch size: 128\n",
    "Epochs: 50\n",
    "Hidden units: 256\n",
    "Activation: relu\n",
    "Optimizer: adam\n",
    "Validation accuracy: 0.9830\n",
    "Dev error: 1.70%\n",
    "\n",
    "Rank 4:\n",
    "Learning rate: 0.01\n",
    "Batch size: 64\n",
    "Epochs: 50\n",
    "Hidden units: 256\n",
    "Activation: relu\n",
    "Optimizer: adam\n",
    "Validation accuracy: 0.9829\n",
    "Dev error: 1.71%\n",
    "\n",
    "Rank 5:\n",
    "Learning rate: 0.0001\n",
    "Batch size: 32\n",
    "Epochs: 50\n",
    "Hidden units: 256\n",
    "Activation: tanh\n",
    "Optimizer: rmsprop\n",
    "Validation accuracy: 0.9828\n",
    "Dev error: 1.72%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2b95be-7250-4ace-83e2-9e5cb178c30a",
   "metadata": {},
   "source": [
    "Old Best 5 hyperparameter choices:\n",
    "\n",
    "Rank 1:\n",
    "Learning rate: 0.01\n",
    "Batch size: 64\n",
    "Epochs: 50\n",
    "Hidden units: 256\n",
    "Activation: relu\n",
    "Optimizer: adam\n",
    "Validation accuracy: 0.984\n",
    "Dev error: 1.59%\n",
    "\n",
    "Rank 2:\n",
    "Learning rate: 0.001\n",
    "Batch size: 128\n",
    "Epochs: 50\n",
    "Hidden units: 256\n",
    "Activation: relu\n",
    "Optimizer: adam\n",
    "Validation accuracy: 0.984\n",
    "Dev error: 1.60%\n",
    "\n",
    "Rank 3:\n",
    "Learning rate: 0.01\n",
    "Batch size: 64\n",
    "Epochs: 50\n",
    "Hidden units: 256\n",
    "Activation: tanh\n",
    "Optimizer: rmsprop\n",
    "Validation accuracy: 0.9831\n",
    "Dev error: 1.69%\n",
    "\n",
    "Rank 4:\n",
    "Learning rate: 0.01\n",
    "Batch size: 32\n",
    "Epochs: 50\n",
    "Hidden units: 256\n",
    "Activation: relu\n",
    "Optimizer: rmsprop\n",
    "Validation accuracy: 0.9827\n",
    "Dev error: 1.73%\n",
    "\n",
    "Rank 5:\n",
    "Learning rate: 0.0001\n",
    "Batch size: 32\n",
    "Epochs: 50\n",
    "Hidden units: 256\n",
    "Activation: relu\n",
    "Optimizer: adam\n",
    "Validation accuracy: 0.9826\n",
    "Dev error: 1.74%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b56057",
   "metadata": {},
   "source": [
    "Compred to the previous benchmark scenario, which achieved a dev error = 2.33%, the best set of hyperparameters here yields an improvement of 0.7%.\n",
    "\n",
    "We now focus on the most accurate architecture of the above and inspect it in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114b49bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Accurate Neural Network (Rank 1)\n",
    "\n",
    "# Hyperparameters 'best' (b)\n",
    "lr_b = 0.01\n",
    "batch_b = 64\n",
    "epoch_b = 50\n",
    "units_b = 256\n",
    "activation_b = 'tanh'\n",
    "optimizer_b = 'rmsprop'\n",
    "\n",
    "# Function to create and compile model\n",
    "def create_model(learning_rate, batch_size, hidden_units, activation, optimizer):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(hidden_units, activation=activation, input_shape=(28*28,)),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile model with given optimizer and learning rate\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model(lr_b, batch_b, units_b, activation_b, optimizer_b)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, \n",
    "                        epochs=epoch_b, \n",
    "                        batch_size=batch_b, \n",
    "                        validation_data=(x_test, y_test), \n",
    "                        verbose=0)\n",
    "\n",
    "\n",
    "    \n",
    "# Evaluate model performance\n",
    "training_error = (1 - history.history['accuracy'][-1])\n",
    "\n",
    "val_accuracy = history.history['val_accuracy'][-1]\n",
    "dev_error = 1 - val_accuracy\n",
    "\n",
    "\n",
    "# Plot the loss function\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Function vs Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final dev accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal training error: {training_error*100} %\")\n",
    "print(f\"Final dev error: {dev_error*100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa5e8e8",
   "metadata": {},
   "source": [
    "While the loss function on the training set converges extremely well, the loss function on the validation set stops decreasing after around 10 epochs, and slightly increases reaching 50 epochs, for all 3 most accurate models. This suggests an overfitting of the training data, which motivates us to include regularization.\n",
    "\n",
    "We now add dropout regularization to the model. Again, we begin by considering a benchmark model with a fixed dropout rate of 0.3, and then scan over various rates to select the values that work best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dde4793c-2b94-46e2-995b-3450a6633b4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Create and train the model\u001b[39;00m\n\u001b[1;32m     26\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model(lr_b, batch_b, units_b, activation_b, optimizer_b)\n\u001b[0;32m---> 28\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Evaluate model performance\u001b[39;00m\n\u001b[1;32m     35\u001b[0m training_error \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    218\u001b[0m     ):\n\u001b[0;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:138\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Bind it ourselves to skip unnecessary canonicalization of default call.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbound_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/tensorflow/core/function/polymorphism/function_type.py:391\u001b[0m, in \u001b[0;36mFunctionType.unpack_inputs\u001b[0;34m(self, bound_parameters)\u001b[0m\n\u001b[1;32m    388\u001b[0m flat \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m sorted_parameters:\n\u001b[1;32m    390\u001b[0m   flat\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m--> 391\u001b[0m       \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_constraint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbound_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m   )\n\u001b[1;32m    394\u001b[0m dealiased_inputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    395\u001b[0m ids_used \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/tensorflow/python/framework/type_spec.py:251\u001b[0m, in \u001b[0;36mTypeSpec.to_tensors\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See TraceType base class for details. Do not override.\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m tensors \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 251\u001b[0m \u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_component_specs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_components\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensors\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/tensorflow/python/util/nest.py:628\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnest.map_structure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmap_structure\u001b[39m(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    544\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \n\u001b[1;32m    546\u001b[0m \u001b[38;5;124;03m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;124;03m    ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModality\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCORE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/tensorflow/python/util/nest_util.py:1065\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \n\u001b[1;32m    970\u001b[0m \u001b[38;5;124;03m- For Modality.CORE: Refer to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;124;03m  ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mCORE:\n\u001b[0;32m-> 1065\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_tf_core_map_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mDATA:\n\u001b[1;32m   1067\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _tf_data_map_structure(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/tensorflow/python/util/nest_util.py:1105\u001b[0m, in \u001b[0;36m_tf_core_map_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m   1101\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m   1104\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m-> 1105\u001b[0m     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m   1106\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites,\n\u001b[1;32m   1107\u001b[0m )\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/tensorflow/python/framework/type_spec.py:252\u001b[0m, in \u001b[0;36mTypeSpec.to_tensors.<locals>.<lambda>\u001b[0;34m(spec, v)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See TraceType base class for details. Do not override.\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m tensors \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    251\u001b[0m nest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m spec, v: tensors\u001b[38;5;241m.\u001b[39mextend(\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m),\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_component_specs,\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_components(value))\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensors\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/tensorflow/python/framework/tensor.py:1082\u001b[0m, in \u001b[0;36mTensorSpec.to_tensors\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mto_tensors\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m   1081\u001b[0m   value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcast(value, trace_type\u001b[38;5;241m.\u001b[39mInternalCastContext())\n\u001b[0;32m-> 1082\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_subtype_of\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1084\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived tensor of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1085\u001b[0m     )\n\u001b[1;32m   1086\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [value]\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/tensorflow/python/framework/tensor_shape.py:1225\u001b[0m, in \u001b[0;36mTensorShape.is_subtype_of\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1222\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;66;03m# A Tensor is a subtype if each corresponding dimension is a subtype.\u001b[39;00m\n\u001b[0;32m-> 1225\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/tensorflow/python/framework/tensor_shape.py:1225\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1222\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;66;03m# A Tensor is a subtype if each corresponding dimension is a subtype.\u001b[39;00m\n\u001b[0;32m-> 1225\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mall\u001b[39m(o \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m s \u001b[38;5;241m==\u001b[39m o \u001b[38;5;28;01mfor\u001b[39;00m s, o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dims, other\u001b[38;5;241m.\u001b[39m_dims))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters 'best' (b) with dropout\n",
    "\n",
    "lr_b = 0.01\n",
    "batch_b = 64\n",
    "epoch_b = 50\n",
    "units_b = 256\n",
    "activation_b = 'tanh'\n",
    "optimizer_b = 'rmsprop'\n",
    "\n",
    "# Function to create and compile model\n",
    "def create_model(learning_rate, batch_size, hidden_units, activation, optimizer):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(hidden_units, activation=activation, input_shape=(28*28,)),\n",
    "        tf.keras.layers.Dropout(0.3),  # Add Dropout layer with a dropout rate of 0.3\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile model with given optimizer and learning rate\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and train the model\n",
    "model = create_model(lr_b, batch_b, units_b, activation_b, optimizer_b)\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=epoch_b, \n",
    "                    batch_size=batch_b, \n",
    "                    validation_data=(x_test, y_test), \n",
    "                    verbose=0)\n",
    "\n",
    "# Evaluate model performance\n",
    "training_error = (1 - history.history['accuracy'][-1])\n",
    "val_accuracy = history.history['val_accuracy'][-1]\n",
    "dev_error = 1 - val_accuracy\n",
    "\n",
    "# Plot the loss function\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Function vs Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final dev accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"\\nFinal training error: {training_error*100:.2f} %\")\n",
    "print(f\"Final dev error: {dev_error*100:.2f} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39dc5f1-d4a1-45d1-bc14-c0d5778e2019",
   "metadata": {},
   "source": [
    "We note a slight improvement in the validation error, from 1.68% of the best benchmark scenario without regularazion down to 1.66% with dropout. We now let the dropout rate vary from 0.1 to 0.6 in steps of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94952a4-a6c8-409e-92ca-0941dcfcc832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters 'best' (b) with dropout\n",
    "lr_b = 0.01\n",
    "batch_b = 64\n",
    "epoch_b = 50\n",
    "units_b = 256\n",
    "activation_b = 'tanh'\n",
    "optimizer_b = 'rmsprop'\n",
    "\n",
    "\n",
    "# Function to create and compile model\n",
    "def create_model_dropout(learning_rate, batch_size, hidden_units, dropout_rate, activation, optimizer):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(hidden_units, activation=activation, input_shape=(28*28,)),\n",
    "        tf.keras.layers.Dropout(dropout_rate),  # Add Dropout layer with a dropout rate of 0.3\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile model with given optimizer and learning rate\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Store results\n",
    "dropout_rates = np.arange(0.1, 0.6, 0.1)  # Dropout rates to test\n",
    "val_errors = []  # Store validation errors\n",
    "training_errors = []  # Store training errors\n",
    "\n",
    "\n",
    "for drop_rate in dropout_rates:\n",
    "    # Create and train the model\n",
    "    print(f\"Testing Dropout Rate: {drop_rate}\")\n",
    "    model = create_model_dropout(lr_b, batch_b, units_b, drop_rate, activation_b, optimizer_b)\n",
    "    \n",
    "    history = model.fit(x_train, y_train, \n",
    "                        epochs=epoch_b, \n",
    "                        batch_size=batch_b, \n",
    "                        validation_data=(x_test, y_test), \n",
    "                        verbose=0)\n",
    "    \n",
    "    # Evaluate model performance\n",
    "    training_error = (1 - history.history['accuracy'][-1])\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    dev_error = 1 - val_accuracy\n",
    "\n",
    "\n",
    "    # Store results\n",
    "    training_errors.append(training_error)\n",
    "    val_errors.append(dev_error)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Dropout Rate: {drop_rate}\")\n",
    "    print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"Final dev accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "    print(f\"Training error: {training_error*100:.2f}%\")\n",
    "    print(f\"Validation error: {dev_error*100:.2f}%\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Plot validation error vs dropout rate\n",
    "plt.plot(dropout_rates, val_errors, label='Validation Error', marker='o')\n",
    "plt.xlabel('Dropout Rate')\n",
    "plt.ylabel('Validation Error')\n",
    "plt.title('Validation Error vs Dropout Rate')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e848c236-f8d8-4da6-8bf7-b9aefb6d5ef4",
   "metadata": {},
   "source": [
    "The validation error achieves the lowest values for dropout rates 0.3 and 0.4. We now zoom around this interval further, testing dropout rates going from 0.25 to 0.45 in steps of 0.02 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92336428-24da-4aac-924d-55a6cedcfe0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters 'best' (b) with dropout\n",
    "lr_b = 0.01\n",
    "batch_b = 64\n",
    "epoch_b = 50\n",
    "units_b = 256\n",
    "activation_b = 'tanh'\n",
    "optimizer_b = 'rmsprop'\n",
    "\n",
    "\n",
    "# Function to create and compile model\n",
    "def create_model_dropout(learning_rate, batch_size, hidden_units, dropout_rate, activation, optimizer):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(hidden_units, activation=activation, input_shape=(28*28,)),\n",
    "        tf.keras.layers.Dropout(dropout_rate),  # Add Dropout layer with a dropout rate of 0.3\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile model with given optimizer and learning rate\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Store results\n",
    "dropout_rates = np.arange(0.25, 0.45, 0.02)  # Dropout rates to test\n",
    "val_errors = []  # Store validation errors\n",
    "training_errors = []  # Store training errors\n",
    "\n",
    "\n",
    "for drop_rate in dropout_rates:\n",
    "    # Create and train the model\n",
    "    print(f\"Testing Dropout Rate: {drop_rate}\")\n",
    "    model = create_model_dropout(lr_b, batch_b, units_b, drop_rate, activation_b, optimizer_b)\n",
    "    \n",
    "    history = model.fit(x_train, y_train, \n",
    "                        epochs=epoch_b, \n",
    "                        batch_size=batch_b, \n",
    "                        validation_data=(x_test, y_test), \n",
    "                        verbose=0)\n",
    "    \n",
    "    # Evaluate model performance\n",
    "    training_error = (1 - history.history['accuracy'][-1])\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    dev_error = 1 - val_accuracy\n",
    "\n",
    "\n",
    "    # Store results\n",
    "    training_errors.append(training_error)\n",
    "    val_errors.append(dev_error)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Dropout Rate: {drop_rate}\")\n",
    "    print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"Final dev accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "    print(f\"Training error: {training_error*100:.2f}%\")\n",
    "    print(f\"Validation error: {dev_error*100:.2f}%\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Plot validation error vs dropout rate\n",
    "plt.plot(dropout_rates, val_errors, label='Validation Error', marker='o')\n",
    "plt.xlabel('Dropout Rate')\n",
    "plt.ylabel('Validation Error')\n",
    "plt.title('Validation Error vs Dropout Rate')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8735fd56-91eb-4247-99b0-409d0d347c31",
   "metadata": {},
   "source": [
    "The best dev accuracy is obtained for a dropout rate of 0.41, which yields a training error of 0.60% and a validation error of 1.56%, yielding significant improvement compared to the analogous NN without dropout which had a dev error od 1.68%.\n",
    "\n",
    "As a last step, we perform a final, scarcer, hyperparameter search with this fixed dropout rate of 0.41, to verify if these adjustments can lead to further improvements on the dev accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecee8cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define ranges for hyperparameters\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.03]\n",
    "hidden_units = [128, 256, 512]\n",
    "batch_sizes = [32, 64, 128]\n",
    "\n",
    "# Iterate over all combinations\n",
    "for lr, units, batch in itertools.product(learning_rates, hidden_units, batch_sizes):\n",
    "    print(f\"Testing: lr={lr}, units={units}, batch_size={batch}\")\n",
    "    model = create_model_dropout(lr, batch, units, dropout_rate=0.41, \n",
    "                                 activation='tanh', optimizer='rmsprop')\n",
    "    \n",
    "    history = model.fit(x_train, y_train,\n",
    "                        epochs=10,  # Use fewer epochs for quick evaluation\n",
    "                        batch_size=batch,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        verbose=0)\n",
    "    \n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    dev_error = 1 - val_accuracy\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation error: {dev_error*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc0eceb-9f47-4690-b920-3b727ec88beb",
   "metadata": {},
   "source": [
    "The best accuracy is obtained for a learning rate = 0.01, 512 units, batch size = 64, which yields a validation accuracy = 0.9851, i.e. a dev error = 1.49%.\n",
    "\n",
    "This represents a substantial improvement compared to the initial benchmark scenario (dev error = 2.33%) and also to the previously chosen \"best scenario\" without dropout (dev error =  1.59%). This highlights the importance of both the choice of hyperparameters and the regularization when selecting a functioning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82456bd-6ef3-4d0d-93b4-a38d6c5849a5",
   "metadata": {},
   "source": [
    "# CNN for MNIST Dataset\n",
    "\n",
    "We now move on to consider a CNN for the same MNIST digit recognition.\n",
    "\n",
    "As we just did for the FCNN, we begin with a benchmark model before employing a more complete randomized hyperparameter search and, possibly, regularization.\n",
    "\n",
    "Following the common practice for image recognition, we employ a sequence of a convolutional layer followed by a max-pooling layer, twice, then followed by two flattened, dense layers and a softmax output.\n",
    "\n",
    "In this benchmark model, each convolutional layer applies ReLU activation and uses a 3×3 filter size to extract local features from the input images. The first convolutional layer contains 32 filters, while the second has 64 filters, allowing the model to progressively learn more complex patterns. Each convolutional layer is followed by a 2×2 max-pooling layer, which reduces spatial dimensions and enhances computational efficiency.\n",
    "\n",
    "After feature extraction, the output is flattened and passed through a fully connected layer with 128 neurons using ReLU activation. Finally, the model concludes with a softmax output layer with 10 neurons, corresponding to the 10 digit classes in the MNIST dataset.\n",
    "\n",
    "The model is optimized using the Adam optimizer, which is well-suited for adaptive learning rate adjustments, and trained using the sparse categorical cross-entropy loss function. The training process spans 10 epochs, with performance tracked on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5583d67-31dc-4e23-8cbf-7494dfa799e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark Convolutional Neural Network (CNN)\n",
    "\n",
    "# Reshape input data for CNN and normalize pixels\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Model architecture\n",
    "model = models.Sequential([\n",
    "    # First convolutional layer\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    # Second convolutional layer\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    # Flatten the output of the convolutional layers\n",
    "    layers.Flatten(),\n",
    "\n",
    "    # Fully connected layer\n",
    "    layers.Dense(128, activation='relu'),\n",
    "\n",
    "    # Output layer (10 classes for MNIST digits)\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model with ADAM optimizer\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model and save the training history\n",
    "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "# Plot the loss function\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Function vs Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(x_test, y_test)\n",
    "\n",
    "training_error = (1 - history.history['accuracy'][-1])\n",
    "dev_error = (1 - history.history['val_accuracy'][-1])\n",
    "\n",
    "print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final dev accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal training error: {training_error*100} %\")\n",
    "print(f\"Final dev error: {dev_error*100} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367f89ab-b418-40e5-80cc-5cae8abfc3a5",
   "metadata": {},
   "source": [
    "This yields:\n",
    "- Final training accuracy: 0.9838\n",
    "- Final dev accuracy: 0.9839\n",
    "- Final training error: 1.623 %\n",
    "- Final dev error: 1.61 %\n",
    "\n",
    "As a semi-random, benchmark model, this already yields a superior performance compared to the initial benchmark FCNN model (dev error = 2.33%) and also virtually no overfitting!\n",
    "\n",
    "We now go further to consider a hyperparameter search. In contrast to the hyperparameter grid used for the FCNN, which was chosen for pedagogical clarity, we now employ a randomized hyperparameter search. This allows us to explore a broader range of hyperparameters while keeping computational costs manageable.\n",
    "\n",
    "Specifically, we tune the number of filters in each convolutional layer, ranging from 32 to 128 in steps of 32, and the kernel size, which is chosen between 3×3 and 5×5. We also experiment with different optimizers (adam and sgd) to assess their impact on training performance. The Hyperband search algorithm efficiently allocates computational resources by pruning weaker models early and focusing training on the most promising configurations.\n",
    "\n",
    "We expect this search to yield an improved validation accuracy compared to our benchmark model, while also identifying an optimal balance between model complexity and generalization. Additionally, tracking the total search time will help us assess whether the added complexity is justified in terms of performance gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c333c68e-89d4-41ff-809e-2f8b6fea7aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized hyperparameter search\n",
    "\n",
    "def build_model(hp):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # First convolutional layer\n",
    "    model.add(layers.Conv2D(\n",
    "        filters=hp.Int('filters1', min_value=32, max_value=128, step=32),\n",
    "        kernel_size=hp.Choice('kernel_size1', values=[3, 5]),  # Use integers here\n",
    "        activation='relu',\n",
    "        input_shape=(28, 28, 1)\n",
    "    ))\n",
    "    model.add(layers.MaxPooling2D(2, 2))\n",
    "\n",
    "    # Second convolutional layer\n",
    "    model.add(layers.Conv2D(\n",
    "        filters=hp.Int('filters2', min_value=32, max_value=128, step=32),\n",
    "        kernel_size=hp.Choice('kernel_size2', values=[3, 5]),  # Use integers here\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(layers.MaxPooling2D(2, 2))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # Fully connected layer\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    # Compile the model with hyperparameters\n",
    "    model.compile(\n",
    "        optimizer=hp.Choice('optimizer', values=['adam', 'sgd']),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Set up the Hyperband search for hyperparameters\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=30,\n",
    "    factor=3,\n",
    "    hyperband_iterations=2,\n",
    "    directory='my_dir',\n",
    "    project_name='cnn_mnist_tuning'\n",
    ")\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the search\n",
    "tuner.search(x_train, y_train, epochs=30, validation_data=(x_test, y_test))\n",
    "\n",
    "# Stop the timer and print the elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for hyperparameter search: {elapsed_time/60:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd280da9-9258-45a2-a5f6-5ff5c6448e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 3 models\n",
    "best_hps = tuner.oracle.get_best_trials(num_trials=3)\n",
    "for i, trial in enumerate(best_hps):\n",
    "    print(f\"Trial {i+1}:\")\n",
    "    print(trial.hyperparameters.values)\n",
    "    print(f\"Score: {trial.score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e4d6b8-f86e-42bc-9f8b-992086fc2d78",
   "metadata": {},
   "source": [
    "Best 3 models:\n",
    "- Trial 1:\n",
    "{'filters1': 32, 'kernel_size1': 5, 'filters2': 128, 'kernel_size2': 5, 'optimizer': 'adam', 'tuner/epochs': 30, 'tuner/initial_epoch': 10, 'tuner/bracket': 1, 'tuner/round': 1, 'tuner/trial_id': '0080'}\n",
    "Score: 0.9893\n",
    "- Trial 2:\n",
    "{'filters1': 32, 'kernel_size1': 3, 'filters2': 128, 'kernel_size2': 5, 'optimizer': 'adam', 'tuner/epochs': 30, 'tuner/initial_epoch': 10, 'tuner/bracket': 1, 'tuner/round': 1, 'tuner/trial_id': '0081'}\n",
    "Score: 0.9885\n",
    "- Trial 3:\n",
    "{'filters1': 128, 'kernel_size1': 5, 'filters2': 128, 'kernel_size2': 3, 'optimizer': 'adam', 'tuner/epochs': 30, 'tuner/initial_epoch': 10, 'tuner/bracket': 3, 'tuner/round': 3, 'tuner/trial_id': '0136'}\n",
    "Score: 0.9885"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f9c178-49ae-4102-963a-af65b2c6d1bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
